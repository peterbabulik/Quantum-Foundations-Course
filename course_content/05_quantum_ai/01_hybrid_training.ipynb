{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Quantum-Classical Neural Networks\n",
    "\n",
    "## Training with PyTorch and PennyLane\n",
    "\n",
    "Hybrid quantum-classical neural networks combine the power of classical deep learning with quantum circuits. The quantum layer can learn patterns that are difficult for classical networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Note: PennyLane would be used for actual quantum circuits\n",
    "# For this demonstration, we simulate the quantum layer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hybrid Architecture\n",
    "\n",
    "A hybrid quantum-classical neural network has:\n",
    "\n",
    "1. **Classical pre-processing**: Classical neural network layers\n",
    "2. **Quantum layer**: Parameterized quantum circuit\n",
    "3. **Classical post-processing**: Final classical layers\n",
    "\n",
    "```\n",
    "Input → [Classical Layers] → [Quantum Layer] → [Classical Layers] → Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedQuantumLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simulated quantum layer for demonstration.\n",
    "\n",
    "    In practice, this would be a PennyLane quantum circuit.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Quantum circuit parameters (rotation angles)\n",
    "        n_params = n_qubits * n_layers * 3  # 3 rotation angles per qubit per layer\n",
    "        self.quantum_params = nn.Parameter(torch.randn(n_params) * 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Simulate quantum circuit processing.\n",
    "\n",
    "        In a real implementation, this would:\n",
    "        1. Encode input data into quantum states\n",
    "        2. Apply parameterized quantum gates\n",
    "        3. Measure expectation values\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Simulate quantum processing with a non-linear transformation\n",
    "        # This mimics the behavior of a variational quantum circuit\n",
    "\n",
    "        # Reshape parameters for processing\n",
    "        params = self.quantum_params.view(self.n_layers, self.n_qubits, 3)\n",
    "\n",
    "        # Apply simulated rotations (simplified)\n",
    "        output = torch.zeros(batch_size, self.n_qubits)\n",
    "\n",
    "        for i in range(self.n_qubits):\n",
    "            # Simulate expectation value measurement\n",
    "            # In reality, this would be <Z_i> after the quantum circuit\n",
    "            angle_sum = torch.sum(torch.abs(params[:, i, :]))\n",
    "            output[:, i] = torch.tanh(x[:, i % x.shape[1]] + angle_sum)\n",
    "\n",
    "        return output\n",
    "\n",
    "class HybridQuantumNeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A hybrid quantum-classical neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, n_qubits, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Classical pre-processing\n",
    "        self.classical_pre = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_qubits),\n",
    "            nn.Tanh()  # Normalize to [-1, 1] for quantum encoding\n",
    "        )\n",
    "\n",
    "        # Quantum layer\n",
    "        self.quantum_layer = SimulatedQuantumLayer(n_qubits)\n",
    "\n",
    "        # Classical post-processing\n",
    "        self.classical_post = nn.Sequential(\n",
    "            nn.Linear(n_qubits, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classical_pre(x)\n",
    "        x = self.quantum_layer(x)\n",
    "        x = self.classical_post(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = HybridQuantumNeuralNetwork(\n",
    "    input_size=4,\n",
    "    hidden_size=16,\n",
    "    n_qubits=4,\n",
    "    output_size=2\n",
    ")\n",
    "\n",
    "print(\"Hybrid Quantum Neural Network Architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Simple Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_samples=500, noise=0.1):\n",
    "    \"\"\"\n",
    "    Create a simple binary classification dataset.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Two concentric circles (like sklearn's make_circles)\n",
    "    n_per_class = n_samples // 2\n",
    "    \n",
    "    # Class 0: inner circle\n",
    "    r0 = np.random.normal(0.3, noise, n_per_class)\n",
    "    theta0 = np.random.uniform(0, 2*np.pi, n_per_class)\n",
    "    X0 = np.column_stack([r0 * np.cos(theta0), r0 * np.sin(theta0)])\n",
    "    \n",
    "    # Class 1: outer circle\n",
    "    r1 = np.random.normal(0.7, noise, n_per_class)\n",
    "    theta1 = np.random.uniform(0, 2*np.pi, n_per_class)\n",
    "    X1 = np.column_stack([r1 * np.cos(theta1), r1 * np.sin(theta1)])\n",
    "    \n",
    "    # Combine\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.hstack([np.zeros(n_per_class), np.ones(n_per_class)])\n",
    "    \n",
    "    # Add extra features (to match input_size=4)\n",
    "    X = np.column_stack([X, np.random.randn(n_samples, 2) * 0.1])\n",
    "    \n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    X, y = X[idx], y[idx]\n",
    "    \n",
    "    return X.astype(np.float32), y.astype(np.int64)\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_dataset()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Class 0', alpha=0.6)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Class 1', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Binary Classification Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X)\n",
    "y_tensor = torch.from_numpy(y)\n",
    "\n",
    "# Split into train/test\n",
    "n_train = int(0.8 * len(X))\n",
    "X_train, X_test = X_tensor[:n_train], X_tensor[n_train:]\n",
    "y_train, y_test = y_tensor[:n_train], y_tensor[n_train:]\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = HybridQuantumNeuralNetwork(\n",
    "    input_size=4,\n",
    "    hidden_size=16,\n",
    "    n_qubits=4,\n",
    "    output_size=2\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "print(\"Training Hybrid Quantum Neural Network...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(train_losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(train_accuracies, 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    \n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of the classifier.\n",
    "    \"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Create grid points\n",
    "    grid = np.c_[xx.ravel(), yy.ravel(), \n",
    "                 np.zeros(xx.ravel().shape), \n",
    "                 np.zeros(xx.ravel().shape)]\n",
    "    grid_tensor = torch.from_numpy(grid.astype(np.float32))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(grid_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1].numpy()\n",
    "    \n",
    "    probs = probs.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, probs, levels=20, cmap='RdBu', alpha=0.8)\n",
    "    plt.colorbar(label='Probability of Class 1')\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Class 0', edgecolors='white')\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Class 1', edgecolors='white')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Decision Boundary')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with Classical-Only Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalNeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A purely classical neural network for comparison.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Train classical model\n",
    "classical_model = ClassicalNeuralNetwork(input_size=4, hidden_size=16, output_size=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classical_model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training Classical Neural Network...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "classical_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    classical_model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classical_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    classical_losses.append(epoch_loss / len(train_loader))\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: Loss = {classical_losses[-1]:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "classical_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classical_model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    classical_accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "\n",
    "print(f\"\\nClassical Model Test Accuracy: {classical_accuracy:.4f}\")\n",
    "print(f\"Hybrid Model Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Hybrid architecture** combines classical and quantum layers\n",
    "2. **Quantum layers** can be trained with backpropagation (parameter-shift rule)\n",
    "3. **Gradients flow** through the quantum circuit just like classical layers\n",
    "4. **Potential advantages**: Quantum layers may learn different patterns\n",
    "\n",
    "**Practical Considerations:**\n",
    "- Real quantum hardware introduces noise and errors\n",
    "- Shot noise (finite samples) affects gradient estimation\n",
    "- Barren plateaus can be a challenge in quantum neural networks\n",
    "- Current advantage is mainly for specific quantum-inspired problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
